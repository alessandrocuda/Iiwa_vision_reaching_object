{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Iiwa_NN_vision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiyrkMIlt0yB",
        "outputId": "676f6756-9664-4eb4-e612-d218ce222cd6"
      },
      "source": [
        "!pip install pickle5"
      ],
      "id": "fiyrkMIlt0yB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 23.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219266 sha256=0c339b5eabb096feabe6e06cb3e431875e12baaa2886270b36cc3092b7bf2551\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muslim-discussion"
      },
      "source": [
        "import pickle5\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "muslim-discussion",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahead-requirement"
      },
      "source": [
        "with open('data.pickle', 'rb') as handle:\n",
        "    data = pickle5.load(handle)"
      ],
      "id": "ahead-requirement",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cshHKkrHGRrQ",
        "outputId": "0acaab28-8c72-44e3-8feb-b7cf5db0c540"
      },
      "source": [
        "data.keys()"
      ],
      "id": "cshHKkrHGRrQ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['X', 'y'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "received-mother",
        "outputId": "08adc37b-4074-4312-cdc6-918440dd571b"
      },
      "source": [
        "total_sample = len(data[\"X\"])\n",
        "print(total_sample)"
      ],
      "id": "received-mother",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "empirical-blame",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01063fda-540d-4e15-c029-9404f575d159"
      },
      "source": [
        "X = np.array(data[\"X\"])\n",
        "y = np.array(data[\"y\"])\n",
        "\n",
        "num_elems = X.shape[0]\n",
        "print(num_elems)\n",
        "print(X[567])\n",
        "print(y[567])"
      ],
      "id": "empirical-blame",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n",
            "[265. 576.]\n",
            "[ 1.11   -0.11    1.0625]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "million-promotion",
        "outputId": "35111a0c-5ac7-47f7-fb47-e3aedc7f62eb"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=y[:,0], y=y[:,1], z=y[:,2],\n",
        "                                   mode='markers',\n",
        "                                      marker=dict(\n",
        "                                        size=2,\n",
        "                                        colorscale='Viridis',   # choose a colorscale\n",
        "                                        opacity=0.8\n",
        "                                    ))])\n",
        "fig.show()"
      ],
      "id": "million-promotion",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a3132ecc-9f5e-4062-a338-5af35c884c48\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a3132ecc-9f5e-4062-a338-5af35c884c48\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a3132ecc-9f5e-4062-a338-5af35c884c48',\n",
              "                        [{\"marker\": {\"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"opacity\": 0.8, \"size\": 2}, \"mode\": \"markers\", \"type\": \"scatter3d\", \"x\": [1.13, 1.2, 1.2, 1.12, 1.52, 1.61, 1.23, 1.09, 1.03, 1.65, 1.22, 1.46, 1.07, 1.15, 1.65, 1.09, 1.09, 1.39, 1.04, 1.12, 1.18, 1.14, 1.65, 1.05, 1.29, 1.29, 1.08, 1.66, 1.59, 1.14, 1.44, 1.46, 1.31, 1.01, 1.55, 1.18, 1.05, 1.18, 1.05, 1.27, 1.04, 1.59, 1.02, 1.65, 1.09, 1.66, 1.62, 1.61, 1.15, 1.47, 1.31, 1.1, 1.2, 1.49, 1.33, 1.38, 1.65, 1.63, 1.62, 1.53, 1.65, 1.3, 1.63, 1.01, 1.2, 1.65, 1.7, 1.69, 1.66, 1.17, 1.21, 1.63, 1.67, 1.45, 1.43, 1.13, 1.5, 1.17, 1.28, 1.67, 1.51, 1.21, 1.12, 1.05, 1.64, 1.54, 1.64, 1.31, 1.55, 1.25, 1.02, 1.23, 1.53, 1.29, 1.45, 1.51, 1.61, 1.44, 1.09, 1.54, 1.6800000000000002, 1.06, 1.63, 1.01, 1.06, 1.02, 1.09, 1.6800000000000002, 1.6800000000000002, 1.09, 1.51, 1.37, 1.23, 1.63, 1.2, 1.53, 1.2, 1.63, 1.44, 1.67, 1.39, 1.49, 1.14, 1.6800000000000002, 1.5, 1.19, 1.19, 1.42, 1.55, 1.64, 1.66, 1.44, 1.24, 1.46, 1.18, 1.13, 1.43, 1.26, 1.17, 1.42, 1.64, 1.53, 1.5699999999999998, 1.47, 1.47, 1.36, 1.23, 1.63, 1.23, 1.36, 1.59, 1.66, 1.46, 1.16, 1.05, 1.24, 1.6800000000000002, 1.55, 1.69, 1.08, 1.28, 1.15, 1.37, 1.16, 1.7, 1.21, 1.44, 1.18, 1.51, 1.11, 1.04, 1.62, 1.03, 1.63, 1.55, 1.22, 1.59, 1.03, 1.2, 1.6, 1.53, 1.54, 1.26, 1.17, 1.07, 1.56, 1.28, 1.29, 1.22, 1.5699999999999998, 1.17, 1.49, 1.04, 1.29, 1.27, 1.19, 1.55, 1.49, 1.17, 1.15, 1.62, 1.6, 1.56, 1.22, 1.17, 1.34, 1.15, 1.06, 1.56, 1.02, 1.35, 1.59, 1.2, 1.23, 1.53, 1.19, 1.3, 1.18, 1.63, 1.45, 1.3, 1.25, 1.17, 1.32, 1.63, 1.16, 1.03, 1.14, 1.2, 1.4, 1.24, 1.64, 1.03, 1.35, 1.15, 1.15, 1.47, 1.1, 1.56, 1.64, 1.49, 1.56, 1.16, 1.18, 1.27, 1.64, 1.06, 1.34, 1.05, 1.05, 1.5, 1.66, 1.27, 1.14, 1.48, 1.1, 1.31, 1.22, 1.06, 1.17, 1.44, 1.08, 1.47, 1.46, 1.5699999999999998, 1.24, 1.6800000000000002, 1.12, 1.15, 1.31, 1.5, 1.01, 1.25, 1.07, 1.54, 1.01, 1.24, 1.47, 1.29, 1.58, 1.21, 1.3, 1.34, 1.41, 1.37, 1.47, 1.13, 1.33, 1.33, 1.61, 1.5, 1.58, 1.04, 1.1, 1.21, 1.12, 1.19, 1.44, 1.14, 1.51, 1.38, 1.47, 1.08, 1.67, 1.49, 1.38, 1.7, 1.14, 1.52, 1.58, 1.46, 1.51, 1.59, 1.01, 1.17, 1.63, 1.69, 1.06, 1.28, 1.14, 1.67, 1.7, 1.47, 1.27, 1.32, 1.33, 1.36, 1.37, 1.37, 1.23, 1.21, 1.66, 1.1, 1.47, 1.6800000000000002, 1.29, 1.28, 1.61, 1.15, 1.16, 1.61, 1.54, 1.58, 1.54, 1.26, 1.5, 1.03, 1.09, 1.15, 1.15, 1.48, 1.42, 1.12, 1.09, 1.44, 1.33, 1.27, 1.5, 1.6800000000000002, 1.53, 1.08, 1.53, 1.42, 1.39, 1.56, 1.53, 1.6, 1.41, 1.69, 1.45, 1.24, 1.43, 1.34, 1.35, 1.2, 1.65, 1.09, 1.13, 1.43, 1.4, 1.08, 1.3, 1.46, 1.13, 1.48, 1.58, 1.04, 1.65, 1.45, 1.46, 1.61, 1.23, 1.27, 1.51, 1.32, 1.38, 1.18, 1.33, 1.15, 1.31, 1.42, 1.11, 1.52, 1.23, 1.28, 1.23, 1.52, 1.27, 1.03, 1.56, 1.29, 1.05, 1.63, 1.54, 1.16, 1.39, 1.33, 1.45, 1.4, 1.23, 1.69, 1.6800000000000002, 1.07, 1.47, 1.59, 1.56, 1.2, 1.32, 1.03, 1.46, 1.25, 1.51, 1.67, 1.26, 1.49, 1.46, 1.07, 1.55, 1.42, 1.21, 1.31, 1.42, 1.55, 1.44, 1.61, 1.34, 1.66, 1.04, 1.53, 1.43, 1.69, 1.62, 1.07, 1.58, 1.59, 1.03, 1.15, 1.35, 1.59, 1.44, 1.33, 1.38, 1.7, 1.03, 1.26, 1.11, 1.5, 1.17, 1.44, 1.16, 1.26, 1.67, 1.35, 1.6, 1.62, 1.33, 1.09, 1.2, 1.43, 1.62, 1.5699999999999998, 1.35, 1.59, 1.49, 1.5, 1.23, 1.48, 1.08, 1.07, 1.64, 1.49, 1.45, 1.51, 1.23, 1.6, 1.58, 1.38, 1.29, 1.3, 1.23, 1.67, 1.09, 1.67, 1.27, 1.69, 1.67, 1.41, 1.67, 1.5, 1.36, 1.46, 1.69, 1.42, 1.52, 1.44, 1.36, 1.27, 1.6, 1.03, 1.08, 1.63, 1.49, 1.34, 1.46, 1.65, 1.44, 1.2, 1.45, 1.5, 1.1, 1.65, 1.46, 1.51, 1.26, 1.6, 1.6800000000000002, 1.66, 1.39, 1.21, 1.67, 1.55, 1.31, 1.65, 1.46, 1.48, 1.22, 1.02, 1.55, 1.05, 1.12, 1.69, 1.16, 1.25, 1.13, 1.55, 1.14, 1.11, 1.19, 1.08, 1.16, 1.48, 1.27, 1.2, 1.33, 1.39, 1.31, 1.09, 1.11, 1.53, 1.19, 1.16, 1.31, 1.38, 1.04, 1.12, 1.35, 1.44, 1.17, 1.33, 1.16, 1.7, 1.25, 1.56, 1.67, 1.23, 1.07, 1.19, 1.51, 1.16, 1.69, 1.1, 1.42, 1.66, 1.29, 1.25, 1.44, 1.34, 1.24, 1.07, 1.29, 1.21, 1.44, 1.21, 1.17, 1.2, 1.46, 1.47, 1.19, 1.37, 1.45, 1.6800000000000002, 1.41, 1.36, 1.21, 1.19, 1.2, 1.14, 1.28, 1.13, 1.08, 1.6, 1.3, 1.59, 1.08, 1.03, 1.58, 1.1, 1.19, 1.19, 1.16, 1.05, 1.28, 1.16, 1.2, 1.16, 1.03, 1.44, 1.54, 1.06, 1.49, 1.64, 1.34, 1.28, 1.43, 1.46, 1.45, 1.64, 1.44, 1.56, 1.43, 1.42, 1.34, 1.27, 1.17, 1.14, 1.49, 1.59, 1.5699999999999998, 1.7, 1.12, 1.27, 1.41, 1.69, 1.6, 1.13, 1.06, 1.58, 1.25, 1.44, 1.1, 1.54, 1.44, 1.66, 1.27, 1.54, 1.26, 1.28, 1.65, 1.45, 1.13, 1.35, 1.6800000000000002, 1.37, 1.23, 1.29, 1.58, 1.62, 1.27, 1.01, 1.31, 1.11, 1.15, 1.67, 1.08, 1.18, 1.46, 1.29, 1.26, 1.23, 1.6800000000000002, 1.5699999999999998, 1.02, 1.56, 1.25, 1.36, 1.5699999999999998, 1.17, 1.06, 1.17, 1.01, 1.7, 1.03, 1.14, 1.43, 1.53, 1.44, 1.29, 1.24, 1.01, 1.7, 1.03, 1.45, 1.23, 1.61, 1.63, 1.67, 1.29, 1.09, 1.04, 1.06, 1.66, 1.46, 1.45, 1.03, 1.5699999999999998, 1.6800000000000002, 1.44, 1.69, 1.6, 1.51, 1.47, 1.39, 1.62, 1.1, 1.43, 1.32, 1.63, 1.24, 1.5, 1.56, 1.12, 1.24, 1.23, 1.09, 1.09, 1.15, 1.64, 1.03, 1.58, 1.47, 1.03, 1.25, 1.1, 1.67, 1.67, 1.31, 1.29, 1.66, 1.08, 1.6, 1.65, 1.36, 1.48, 1.64, 1.52, 1.32, 1.55, 1.29, 1.07, 1.54, 1.34, 1.38, 1.17, 1.14, 1.62, 1.56, 1.54, 1.4, 1.45, 1.32, 1.7, 1.47, 1.04, 1.65, 1.02, 1.51, 1.58, 1.55, 1.34, 1.23, 1.23, 1.26, 1.34, 1.61, 1.58, 1.23, 1.69, 1.54, 1.1, 1.43, 1.67, 1.26, 1.23, 1.12, 1.02, 1.58, 1.53, 1.45, 1.61, 1.17, 1.64, 1.26, 1.27, 1.64, 1.28, 1.13, 1.47, 1.67, 1.39, 1.38, 1.33, 1.35, 1.64, 1.21, 1.59, 1.49, 1.6, 1.58, 1.25, 1.35, 1.19, 1.43, 1.5, 1.6800000000000002, 1.3, 1.44, 1.13, 1.08, 1.42, 1.61, 1.55, 1.64, 1.66, 1.11, 1.44, 1.22, 1.62, 1.28, 1.59, 1.27, 1.36, 1.25, 1.03, 1.22, 1.08, 1.02, 1.42, 1.13, 1.61, 1.41, 1.11, 1.6800000000000002, 1.38, 1.59, 1.47, 1.26, 1.29, 1.32, 1.69, 1.69, 1.43, 1.22, 1.12, 1.25, 1.33, 1.11, 1.19, 1.18, 1.2, 1.59, 1.1, 1.56, 1.09, 1.42, 1.24, 1.17, 1.54, 1.09, 1.21, 1.17, 1.48, 1.55, 1.08, 1.49, 1.54, 1.09, 1.12, 1.63, 1.12, 1.35, 1.21, 1.25, 1.09, 1.06, 1.3, 1.28, 1.27, 1.69, 1.52, 1.65, 1.35, 1.67, 1.48, 1.13, 1.5, 1.65, 1.34, 1.37, 1.5, 1.1, 1.14, 1.03, 1.63, 1.17, 1.28, 1.69, 1.38, 1.55, 1.63, 1.53, 1.09, 1.33, 1.05, 1.09, 1.43, 1.04, 1.5699999999999998, 1.6800000000000002, 1.4, 1.51, 1.4, 1.02, 1.27, 1.12, 1.31, 1.21, 1.65, 1.24, 1.32, 1.32, 1.67, 1.66, 1.52, 1.69, 1.1, 1.33, 1.54, 1.69, 1.56, 1.7, 1.51, 1.45, 1.52, 1.59, 1.36, 1.14, 1.28, 1.25, 1.59, 1.47, 1.1, 1.46, 1.16, 1.32, 1.65, 1.47, 1.19, 1.09, 1.54, 1.14, 1.01, 1.23, 1.65, 1.35, 1.58, 1.29, 1.37, 1.3, 1.04], \"y\": [0.23, 0.10000000000000007, -0.93, -0.34, -0.86, -0.24, -0.31999999999999995, -0.040000000000000036, 0.24, 0.020000000000000014, -0.79, 0.040000000000000036, -0.98, 0.2, -0.11, 0.040000000000000036, -0.97, -0.6599999999999999, 0.16999999999999993, 0.1100000000000001, -0.93, -0.42, -0.25, 0.17999999999999994, -0.81, 0.1200000000000001, 0.10000000000000007, 0.25, -0.27, -0.84, -0.07999999999999996, -0.21, -1.0, 0.1499999999999999, -0.2, -0.93, -0.99, -0.4, -0.33, 0.30000000000000004, -0.16000000000000006, -0.16000000000000006, -0.76, -0.4300000000000001, -0.76, -0.13, -0.35, -0.6, -0.94, -0.52, 0.15999999999999992, -0.22, -0.63, -0.2, 0.28, -0.88, 0.05000000000000005, -0.74, -0.33, -0.4, 0.06000000000000005, -0.98, -0.4, -0.27, -0.22, -0.17000000000000004, -0.36, 0.23, -0.41, -0.08999999999999997, 0.17999999999999994, 0.010000000000000007, 0.1200000000000001, -0.83, -0.24, -0.18999999999999995, 0.18999999999999995, -0.55, -0.91, -0.31000000000000005, -0.79, -0.62, 0.1499999999999999, 0.1200000000000001, -0.76, -0.94, 0.26, -0.73, -0.77, -0.6, -0.99, -0.89, -0.48, -0.52, -0.9, -0.6599999999999999, -1.0, 0.1299999999999999, -0.75, -0.4, -0.12, -0.96, -0.84, -0.5700000000000001, -0.31000000000000005, -0.22, -0.94, -0.06999999999999995, -0.14, 0.17999999999999994, -0.5900000000000001, 0.030000000000000027, -0.61, -0.8200000000000001, 0.22, -0.96, -0.69, -0.9, 0.28, -0.24, -0.88, -0.99, -0.85, 0.10000000000000007, -0.030000000000000027, -0.83, 0.16999999999999993, 0.05000000000000005, 0.18999999999999995, 0.15999999999999992, -0.05000000000000005, -0.010000000000000007, -0.61, 0.040000000000000036, -0.040000000000000036, -0.2, 0.08000000000000007, -0.92, 0.29000000000000004, -0.18999999999999995, -0.5800000000000001, -1.0, -0.94, -0.74, 0.0, -0.34, 0.1299999999999999, -0.53, -0.76, -0.8, -0.2, 0.08000000000000007, -0.84, -0.51, -0.06000000000000005, -0.7, -0.83, -0.41, 0.25, -0.16000000000000006, -0.5800000000000001, -0.88, -0.42, -0.55, -0.34, 0.28, -0.13, -0.47, -0.52, -0.42, -0.97, -0.08999999999999997, -0.74, -0.48, -0.97, -0.020000000000000014, -0.44, -0.28, -0.83, -0.94, -0.75, 0.16999999999999993, -0.64, 0.020000000000000014, -0.91, -0.21, -0.84, 0.05000000000000005, 0.030000000000000027, 0.2, -0.61, -0.040000000000000036, -0.11, -0.18000000000000005, -0.21, -0.65, -0.6799999999999999, -0.87, -0.88, -0.95, -0.52, -0.29000000000000004, -0.73, -0.49, -0.54, -0.17000000000000004, -0.6799999999999999, -0.73, -0.96, -0.09999999999999998, -0.07999999999999996, -0.95, -0.71, -0.46, -0.27, 0.25, -0.15000000000000002, -0.23, -0.33, 0.06000000000000005, -0.79, -0.30000000000000004, 0.18999999999999995, 0.17999999999999994, 0.2, -0.09999999999999998, 0.040000000000000036, 0.25, -0.010000000000000007, -0.69, -0.49, -0.65, 0.09000000000000008, -0.8, -0.42, -0.44, -0.06000000000000005, 0.24, -0.77, -0.34, -0.83, -0.26, 0.020000000000000014, -0.92, -0.63, -0.05000000000000005, 0.22, -0.23, -0.06999999999999995, -0.95, -0.37, -0.44, -0.31999999999999995, -0.8, 0.0, -0.52, 0.030000000000000027, -0.5800000000000001, -0.31000000000000005, -0.35, -0.05000000000000005, -0.45, -0.12, -1.0, 0.30000000000000004, -0.38, -0.16000000000000006, -0.56, 0.06000000000000005, -0.51, -0.63, -0.35, -0.81, -0.77, -0.87, -0.08999999999999997, -0.74, -0.28, 0.1499999999999999, -0.040000000000000036, -0.72, 0.21, 0.09000000000000008, -0.21, -0.25, 0.27, 0.10000000000000007, -0.69, -0.5800000000000001, -0.76, -0.71, 0.1299999999999999, -0.81, -0.46, -0.55, -0.81, -0.22, 0.06000000000000005, 0.030000000000000027, -0.72, -0.6799999999999999, 0.17999999999999994, -0.26, 0.08000000000000007, -0.73, -0.22, -0.95, -0.9, -0.42, -0.53, 0.24, -0.46, -0.36, 0.05000000000000005, -0.46, 0.06000000000000005, 0.26, -0.51, -0.25, -0.97, -0.86, -0.30000000000000004, -0.5700000000000001, -0.31999999999999995, -0.14, 0.1100000000000001, -0.93, -0.97, -0.47, -0.63, 0.07000000000000006, -0.040000000000000036, -0.18000000000000005, -0.27, -0.71, -0.56, -0.78, -0.8, -0.5, -0.14, 0.07000000000000006, -0.33, 0.18999999999999995, -0.76, -0.5, -0.49, -1.0, -0.53, -0.53, -0.4300000000000001, -0.48, -0.06000000000000005, -0.16000000000000006, -0.86, -0.18999999999999995, -0.74, 0.05000000000000005, -0.2, -0.96, -0.5800000000000001, -0.4300000000000001, -0.89, -0.95, -0.23, -0.39, -0.34, -0.88, 0.010000000000000007, 0.1299999999999999, -0.13, -0.62, -0.24, 0.1200000000000001, -0.78, -0.09999999999999998, -0.23, -0.16000000000000006, -0.6599999999999999, -0.16000000000000006, -0.77, -0.6799999999999999, -0.5700000000000001, -0.92, -0.41, -0.7, -0.54, -0.4, 0.24, -0.4300000000000001, -0.69, 0.2, 0.05000000000000005, -0.24, -0.18000000000000005, -0.8200000000000001, -0.06000000000000005, -0.4300000000000001, -0.63, -0.73, 0.23, -0.77, 0.09000000000000008, -0.4, 0.08000000000000007, -0.17000000000000004, -0.48, -0.30000000000000004, -0.97, -0.77, 0.08000000000000007, -0.6599999999999999, -0.84, -0.88, -1.0, -0.21, -0.5900000000000001, -0.05000000000000005, 0.26, -0.23, 0.1499999999999999, -0.74, -0.31000000000000005, 0.16999999999999993, -0.07999999999999996, 0.1299999999999999, -0.37, -0.77, -0.51, 0.07000000000000006, -0.44, -0.93, -0.63, -0.46, 0.18999999999999995, -0.21, -0.26, -0.76, -0.23, -0.5900000000000001, -0.18999999999999995, 0.2, -0.75, -0.6699999999999999, -0.22, -0.37, 0.29000000000000004, -0.33, -0.5900000000000001, -0.5700000000000001, -0.29000000000000004, -0.2, -0.35, -0.6599999999999999, -1.0, -0.94, -0.36, -0.42, 0.27, -0.98, -0.74, -0.71, -0.83, -0.16000000000000006, -0.21, 0.23, 0.1299999999999999, -0.24, -0.13, 0.08000000000000007, -0.81, -0.37, -0.35, -0.98, 0.25, -0.39, 0.1399999999999999, 0.040000000000000036, 0.29000000000000004, -0.78, -0.9, -0.6, 0.1100000000000001, 0.30000000000000004, -0.34, -0.5700000000000001, -0.86, -0.42, -0.79, -0.84, -0.97, -0.81, -0.010000000000000007, -0.5, -0.75, -0.5, -0.14, 0.1399999999999999, -0.81, -0.25, -0.54, -0.15000000000000002, -0.33, -0.2, 0.09000000000000008, 0.25, -0.2, -0.6, -0.7, -0.31000000000000005, 0.10000000000000007, -0.93, -0.29000000000000004, -0.94, 0.30000000000000004, 0.15999999999999992, -0.47, 0.030000000000000027, -0.7, -0.81, -0.99, -0.44, -0.29000000000000004, -0.28, -0.2, -0.18999999999999995, -0.08999999999999997, -0.4, 0.010000000000000007, -0.81, 0.18999999999999995, -0.69, 0.05000000000000005, -0.18000000000000005, -0.22, 0.08000000000000007, 0.010000000000000007, -0.77, 0.27, 0.29000000000000004, -0.75, -0.07999999999999996, -0.76, -0.52, -0.8200000000000001, -0.45, -0.78, 0.06000000000000005, 0.26, -0.86, -0.39, 0.05000000000000005, 0.23, -0.010000000000000007, -0.35, -0.47, -0.23, -0.64, -0.65, -0.93, -0.05000000000000005, -0.84, -0.85, -0.6, -0.96, -0.16000000000000006, 0.020000000000000014, 0.0, -0.11, -0.49, -0.5800000000000001, -0.16000000000000006, -0.77, -0.74, -0.24, -0.30000000000000004, -0.15000000000000002, -0.99, 0.27, -0.040000000000000036, -0.13, -0.6599999999999999, -0.76, -0.73, -0.5800000000000001, -0.31999999999999995, 0.0, 0.1299999999999999, 0.24, -0.24, -0.84, 0.16999999999999993, -0.040000000000000036, -0.73, -0.37, -0.7, -0.85, -0.8, -0.7, 0.27, 0.18999999999999995, -0.47, -0.31999999999999995, -0.81, -0.72, -0.29000000000000004, -0.24, -0.52, -0.8, -0.34, 0.21, 0.010000000000000007, -0.71, -0.89, -0.27, -0.22, 0.17999999999999994, -0.42, -0.44, -0.44, -0.72, -0.88, -0.6699999999999999, -0.6699999999999999, -0.18000000000000005, -0.18000000000000005, 0.05000000000000005, -0.16000000000000006, 0.23, -0.65, -0.5900000000000001, -0.95, -0.16000000000000006, -0.37, -0.41, -0.44, -0.26, -0.16000000000000006, -0.79, -0.51, -0.73, 0.29000000000000004, -0.2, 0.24, -0.37, -0.09999999999999998, 0.28, -0.61, -0.37, 0.1299999999999999, -0.52, -0.87, 0.26, 0.09000000000000008, -0.76, -0.46, -0.35, -0.33, -0.79, -0.11, -0.020000000000000014, 0.29000000000000004, -0.25, 0.25, -0.6799999999999999, -0.020000000000000014, 0.09000000000000008, -0.93, 0.26, 0.07000000000000006, -0.07999999999999996, 0.07000000000000006, -0.87, -0.31999999999999995, -0.5, -0.06000000000000005, 0.23, -0.69, -0.09999999999999998, -0.8200000000000001, -0.55, -0.83, -0.4300000000000001, -0.89, -0.5700000000000001, -0.52, -0.72, -0.13, -0.16000000000000006, -0.6699999999999999, -0.25, -0.49, -0.46, -0.6699999999999999, -0.010000000000000007, -0.18999999999999995, -0.9, -0.52, -0.95, -0.45, -0.4, -0.65, -0.56, 0.05000000000000005, -0.41, -0.36, 0.1100000000000001, -0.92, 0.22, -0.17000000000000004, 0.1499999999999999, -0.51, -0.65, -0.87, -0.74, 0.1399999999999999, 0.05000000000000005, -0.63, -0.42, -0.5, -0.6799999999999999, 0.10000000000000007, -0.44, -0.4, -0.5700000000000001, -0.54, 0.18999999999999995, -0.33, -0.51, 0.10000000000000007, 0.16999999999999993, 0.18999999999999995, -0.06000000000000005, -0.9, -0.47, -0.05000000000000005, -0.28, -0.44, -0.2, 0.2, -0.040000000000000036, 0.17999999999999994, -0.78, 0.23, -0.28, -0.69, -0.06999999999999995, -0.31000000000000005, 0.16999999999999993, -0.30000000000000004, -0.53, -0.5, 0.27, -0.86, -0.97, 0.16999999999999993, 0.24, 0.1200000000000001, 0.020000000000000014, -0.47, -0.92, -0.92, -0.63, -0.73, -0.76, -0.69, 0.27, -0.020000000000000014, -0.6699999999999999, -0.6699999999999999, -0.8, 0.05000000000000005, -0.65, 0.07000000000000006, -0.73, -0.91, 0.10000000000000007, -0.06000000000000005, -0.46, -0.18999999999999995, 0.26, -0.07999999999999996, 0.16999999999999993, 0.09000000000000008, -0.92, -0.23, 0.08000000000000007, 0.1100000000000001, -0.22, -0.95, -0.30000000000000004, 0.10000000000000007, -0.040000000000000036, -0.39, -0.21, 0.17999999999999994, -0.78, 0.25, -0.87, 0.18999999999999995, -0.010000000000000007, -0.14, -0.12, 0.1399999999999999, -0.26, -0.030000000000000027, 0.29000000000000004, -0.56, -0.29000000000000004, -0.18000000000000005, -0.85, 0.1499999999999999, -0.75, -0.41, -0.020000000000000014, 0.1200000000000001, -0.89, 0.28, 0.24, -0.85, -0.23, 0.17999999999999994, -0.06000000000000005, -0.36, 0.0, 0.1499999999999999, -0.83, -0.84, 0.020000000000000014, -0.18999999999999995, -0.92, -0.49, -0.98, -0.39, -0.7, 0.23, -0.74, -0.46, 0.22, -0.6599999999999999, -0.92, -0.5, 0.10000000000000007, -0.06999999999999995, -0.22, -0.31000000000000005, -0.39, -0.5700000000000001, 0.1100000000000001, -0.38, 0.25, -0.71, -0.97, -0.28, -0.020000000000000014, 0.030000000000000027, -0.030000000000000027, -0.44, -0.09999999999999998, -0.18999999999999995, 0.1200000000000001, -0.8200000000000001, -0.77, -0.81, -0.040000000000000036, -0.63, -0.62, 0.1299999999999999, -0.8, -0.77, -0.6, -0.98, -0.07999999999999996, -0.18000000000000005, -0.69, 0.21, -0.56, -0.64, -0.87, -0.53, 0.08000000000000007, -0.31999999999999995, 0.29000000000000004, 0.07000000000000006, -0.09999999999999998, -0.6799999999999999, 0.27, -0.18999999999999995, 0.09000000000000008, -0.71, 0.16999999999999993, 0.23, 0.1200000000000001, 0.21, -0.39, -0.55, -0.31999999999999995, -0.06000000000000005, -0.18000000000000005, -0.85, -0.74, 0.27, -0.4300000000000001, -0.6699999999999999, -0.15000000000000002, 0.06000000000000005, -0.34, -0.6, 0.0, -0.2, -0.83, -0.05000000000000005, -0.30000000000000004, -0.81, 0.23, 0.010000000000000007, -0.4300000000000001, -0.06000000000000005, -0.71, -0.18999999999999995, 0.040000000000000036, -0.69, -0.84, -0.81, -0.5700000000000001, -0.84, -0.35, -0.86, -0.79, -0.47, -0.7, -0.010000000000000007, -0.24, -0.26, -0.44, 0.24, -0.69, -0.9, 0.2, -0.08999999999999997, -0.010000000000000007, -0.73, -0.13, -0.61, -0.49, 0.23, 0.29000000000000004, -0.030000000000000027, 0.15999999999999992, -0.17000000000000004, -0.6799999999999999, -0.8200000000000001, -0.040000000000000036, -0.15000000000000002, 0.28, -0.89, 0.22, -0.5700000000000001, 0.18999999999999995, -0.63, -0.53, -0.44, -0.12, -0.77, -0.88, -0.11, 0.09000000000000008, 0.1100000000000001, -0.010000000000000007, -0.36, 0.06000000000000005, 0.30000000000000004, 0.30000000000000004, 0.040000000000000036, -0.42, -0.52, 0.0, 0.1200000000000001, -0.6799999999999999, -0.92, 0.10000000000000007, -0.81, -0.07999999999999996, -0.53, 0.30000000000000004, 0.21, -0.54, -0.72, -0.69, -0.62, 0.020000000000000014, -0.79, -0.24, -0.5900000000000001, -0.7, -0.08999999999999997, 0.030000000000000027, -0.15000000000000002, -0.45, 0.1100000000000001], \"z\": [1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625, 1.0625]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a3132ecc-9f5e-4062-a338-5af35c884c48');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_LQJXXfJp2q"
      },
      "source": [
        "# Split dataset"
      ],
      "id": "Z_LQJXXfJp2q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVRgKoF2f4FA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=128)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.09, shuffle=True, random_state=128)"
      ],
      "id": "jVRgKoF2f4FA",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "outstanding-pillow",
        "outputId": "6ef87b38-c43a-415e-adf4-255002ff196e"
      },
      "source": [
        "X_train.shape"
      ],
      "id": "outstanding-pillow",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(899, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KlCSMQJHBTA",
        "outputId": "848f6c49-f820-4b12-b47a-25886331bebe"
      },
      "source": [
        "y"
      ],
      "id": "7KlCSMQJHBTA",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.13  ,  0.23  ,  1.0625],\n",
              "       [ 1.2   ,  0.1   ,  1.0625],\n",
              "       [ 1.2   , -0.93  ,  1.0625],\n",
              "       ...,\n",
              "       [ 1.37  , -0.15  ,  1.0625],\n",
              "       [ 1.3   , -0.45  ,  1.0625],\n",
              "       [ 1.04  ,  0.11  ,  1.0625]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdZyLB_J5Bz"
      },
      "source": [
        "# NN vision model"
      ],
      "id": "fVdZyLB_J5Bz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dated-parade"
      },
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(Dense(3))\n",
        "    sgd = tf.optimizers.SGD(0.001, momentum = 0.3, nesterov = True)\n",
        "    adam = tf.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
        "    return model"
      ],
      "id": "dated-parade",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G7eIxaXibAt"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "import numpy as np\n",
        "\n",
        "class ReturnBestEarlyStopping(EarlyStopping):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            if self.verbose > 0:\n",
        "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
        "        elif self.restore_best_weights:\n",
        "            if self.verbose > 0:\n",
        "                print('Restoring model weights from the end of the best epoch.')\n",
        "            self.model.set_weights(self.best_weights)"
      ],
      "id": "3G7eIxaXibAt",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spare-board",
        "scrolled": true,
        "outputId": "d1074746-6723-4cba-e616-b6a729d3a626"
      },
      "source": [
        "# train the model using SGD\n",
        "print(\"[INFO] training network...\")\n",
        "model = create_model()\n",
        "best_callback = ReturnBestEarlyStopping(monitor=\"val_loss\", patience=50, verbose=0, mode=\"min\", restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1000, callbacks=[best_callback], verbose = 2)"
      ],
      "id": "spare-board",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/1000\n",
            "26/26 - 0s - loss: 428.2440 - val_loss: 9.4368\n",
            "Epoch 2/1000\n",
            "26/26 - 0s - loss: 16.2505 - val_loss: 3.4398\n",
            "Epoch 3/1000\n",
            "26/26 - 0s - loss: 1.8493 - val_loss: 1.1224\n",
            "Epoch 4/1000\n",
            "26/26 - 0s - loss: 0.9063 - val_loss: 0.7276\n",
            "Epoch 5/1000\n",
            "26/26 - 0s - loss: 0.6243 - val_loss: 0.5103\n",
            "Epoch 6/1000\n",
            "26/26 - 0s - loss: 0.4926 - val_loss: 0.3906\n",
            "Epoch 7/1000\n",
            "26/26 - 0s - loss: 0.4055 - val_loss: 0.3577\n",
            "Epoch 8/1000\n",
            "26/26 - 0s - loss: 0.3452 - val_loss: 0.2926\n",
            "Epoch 9/1000\n",
            "26/26 - 0s - loss: 0.2977 - val_loss: 0.2733\n",
            "Epoch 10/1000\n",
            "26/26 - 0s - loss: 0.2762 - val_loss: 0.2196\n",
            "Epoch 11/1000\n",
            "26/26 - 0s - loss: 0.2524 - val_loss: 0.2373\n",
            "Epoch 12/1000\n",
            "26/26 - 0s - loss: 0.2294 - val_loss: 0.2125\n",
            "Epoch 13/1000\n",
            "26/26 - 0s - loss: 0.2088 - val_loss: 0.1880\n",
            "Epoch 14/1000\n",
            "26/26 - 0s - loss: 0.1925 - val_loss: 0.1704\n",
            "Epoch 15/1000\n",
            "26/26 - 0s - loss: 0.1834 - val_loss: 0.1652\n",
            "Epoch 16/1000\n",
            "26/26 - 0s - loss: 0.1680 - val_loss: 0.1672\n",
            "Epoch 17/1000\n",
            "26/26 - 0s - loss: 0.1623 - val_loss: 0.1640\n",
            "Epoch 18/1000\n",
            "26/26 - 0s - loss: 0.1584 - val_loss: 0.1588\n",
            "Epoch 19/1000\n",
            "26/26 - 0s - loss: 0.1492 - val_loss: 0.1450\n",
            "Epoch 20/1000\n",
            "26/26 - 0s - loss: 0.1558 - val_loss: 0.1559\n",
            "Epoch 21/1000\n",
            "26/26 - 0s - loss: 0.1446 - val_loss: 0.1432\n",
            "Epoch 22/1000\n",
            "26/26 - 0s - loss: 0.1485 - val_loss: 0.1501\n",
            "Epoch 23/1000\n",
            "26/26 - 0s - loss: 0.1336 - val_loss: 0.1393\n",
            "Epoch 24/1000\n",
            "26/26 - 0s - loss: 0.1279 - val_loss: 0.1299\n",
            "Epoch 25/1000\n",
            "26/26 - 0s - loss: 0.1260 - val_loss: 0.1450\n",
            "Epoch 26/1000\n",
            "26/26 - 0s - loss: 0.1270 - val_loss: 0.1360\n",
            "Epoch 27/1000\n",
            "26/26 - 0s - loss: 0.1203 - val_loss: 0.1314\n",
            "Epoch 28/1000\n",
            "26/26 - 0s - loss: 0.1273 - val_loss: 0.1233\n",
            "Epoch 29/1000\n",
            "26/26 - 0s - loss: 0.1256 - val_loss: 0.1295\n",
            "Epoch 30/1000\n",
            "26/26 - 0s - loss: 0.1165 - val_loss: 0.1186\n",
            "Epoch 31/1000\n",
            "26/26 - 0s - loss: 0.1256 - val_loss: 0.1639\n",
            "Epoch 32/1000\n",
            "26/26 - 0s - loss: 0.1233 - val_loss: 0.1337\n",
            "Epoch 33/1000\n",
            "26/26 - 0s - loss: 0.1130 - val_loss: 0.1368\n",
            "Epoch 34/1000\n",
            "26/26 - 0s - loss: 0.1141 - val_loss: 0.1158\n",
            "Epoch 35/1000\n",
            "26/26 - 0s - loss: 0.1116 - val_loss: 0.1189\n",
            "Epoch 36/1000\n",
            "26/26 - 0s - loss: 0.1179 - val_loss: 0.1073\n",
            "Epoch 37/1000\n",
            "26/26 - 0s - loss: 0.1072 - val_loss: 0.1312\n",
            "Epoch 38/1000\n",
            "26/26 - 0s - loss: 0.1223 - val_loss: 0.1066\n",
            "Epoch 39/1000\n",
            "26/26 - 0s - loss: 0.1080 - val_loss: 0.1178\n",
            "Epoch 40/1000\n",
            "26/26 - 0s - loss: 0.1008 - val_loss: 0.1319\n",
            "Epoch 41/1000\n",
            "26/26 - 0s - loss: 0.1189 - val_loss: 0.1247\n",
            "Epoch 42/1000\n",
            "26/26 - 0s - loss: 0.1255 - val_loss: 0.1153\n",
            "Epoch 43/1000\n",
            "26/26 - 0s - loss: 0.0950 - val_loss: 0.1012\n",
            "Epoch 44/1000\n",
            "26/26 - 0s - loss: 0.1095 - val_loss: 0.1382\n",
            "Epoch 45/1000\n",
            "26/26 - 0s - loss: 0.1047 - val_loss: 0.0967\n",
            "Epoch 46/1000\n",
            "26/26 - 0s - loss: 0.1030 - val_loss: 0.0950\n",
            "Epoch 47/1000\n",
            "26/26 - 0s - loss: 0.0917 - val_loss: 0.0946\n",
            "Epoch 48/1000\n",
            "26/26 - 0s - loss: 0.0902 - val_loss: 0.1129\n",
            "Epoch 49/1000\n",
            "26/26 - 0s - loss: 0.0946 - val_loss: 0.0942\n",
            "Epoch 50/1000\n",
            "26/26 - 0s - loss: 0.1026 - val_loss: 0.0984\n",
            "Epoch 51/1000\n",
            "26/26 - 0s - loss: 0.0880 - val_loss: 0.0909\n",
            "Epoch 52/1000\n",
            "26/26 - 0s - loss: 0.0882 - val_loss: 0.1237\n",
            "Epoch 53/1000\n",
            "26/26 - 0s - loss: 0.0999 - val_loss: 0.0838\n",
            "Epoch 54/1000\n",
            "26/26 - 0s - loss: 0.0863 - val_loss: 0.0831\n",
            "Epoch 55/1000\n",
            "26/26 - 0s - loss: 0.0849 - val_loss: 0.0857\n",
            "Epoch 56/1000\n",
            "26/26 - 0s - loss: 0.0818 - val_loss: 0.1005\n",
            "Epoch 57/1000\n",
            "26/26 - 0s - loss: 0.0788 - val_loss: 0.1272\n",
            "Epoch 58/1000\n",
            "26/26 - 0s - loss: 0.0759 - val_loss: 0.0732\n",
            "Epoch 59/1000\n",
            "26/26 - 0s - loss: 0.0789 - val_loss: 0.0820\n",
            "Epoch 60/1000\n",
            "26/26 - 0s - loss: 0.0740 - val_loss: 0.0906\n",
            "Epoch 61/1000\n",
            "26/26 - 0s - loss: 0.0967 - val_loss: 0.0831\n",
            "Epoch 62/1000\n",
            "26/26 - 0s - loss: 0.0860 - val_loss: 0.1166\n",
            "Epoch 63/1000\n",
            "26/26 - 0s - loss: 0.0685 - val_loss: 0.0684\n",
            "Epoch 64/1000\n",
            "26/26 - 0s - loss: 0.0604 - val_loss: 0.0681\n",
            "Epoch 65/1000\n",
            "26/26 - 0s - loss: 0.0809 - val_loss: 0.0782\n",
            "Epoch 66/1000\n",
            "26/26 - 0s - loss: 0.0760 - val_loss: 0.0769\n",
            "Epoch 67/1000\n",
            "26/26 - 0s - loss: 0.0634 - val_loss: 0.0667\n",
            "Epoch 68/1000\n",
            "26/26 - 0s - loss: 0.0707 - val_loss: 0.0617\n",
            "Epoch 69/1000\n",
            "26/26 - 0s - loss: 0.0734 - val_loss: 0.0794\n",
            "Epoch 70/1000\n",
            "26/26 - 0s - loss: 0.0659 - val_loss: 0.0972\n",
            "Epoch 71/1000\n",
            "26/26 - 0s - loss: 0.0639 - val_loss: 0.0583\n",
            "Epoch 72/1000\n",
            "26/26 - 0s - loss: 0.0669 - val_loss: 0.0782\n",
            "Epoch 73/1000\n",
            "26/26 - 0s - loss: 0.0561 - val_loss: 0.0942\n",
            "Epoch 74/1000\n",
            "26/26 - 0s - loss: 0.0726 - val_loss: 0.0729\n",
            "Epoch 75/1000\n",
            "26/26 - 0s - loss: 0.0656 - val_loss: 0.0512\n",
            "Epoch 76/1000\n",
            "26/26 - 0s - loss: 0.0494 - val_loss: 0.0481\n",
            "Epoch 77/1000\n",
            "26/26 - 0s - loss: 0.0675 - val_loss: 0.0783\n",
            "Epoch 78/1000\n",
            "26/26 - 0s - loss: 0.0572 - val_loss: 0.0654\n",
            "Epoch 79/1000\n",
            "26/26 - 0s - loss: 0.0538 - val_loss: 0.0396\n",
            "Epoch 80/1000\n",
            "26/26 - 0s - loss: 0.0503 - val_loss: 0.0574\n",
            "Epoch 81/1000\n",
            "26/26 - 0s - loss: 0.0595 - val_loss: 0.0481\n",
            "Epoch 82/1000\n",
            "26/26 - 0s - loss: 0.0495 - val_loss: 0.0516\n",
            "Epoch 83/1000\n",
            "26/26 - 0s - loss: 0.0448 - val_loss: 0.0400\n",
            "Epoch 84/1000\n",
            "26/26 - 0s - loss: 0.0451 - val_loss: 0.0329\n",
            "Epoch 85/1000\n",
            "26/26 - 0s - loss: 0.0405 - val_loss: 0.0494\n",
            "Epoch 86/1000\n",
            "26/26 - 0s - loss: 0.0482 - val_loss: 0.0873\n",
            "Epoch 87/1000\n",
            "26/26 - 0s - loss: 0.0476 - val_loss: 0.0334\n",
            "Epoch 88/1000\n",
            "26/26 - 0s - loss: 0.0404 - val_loss: 0.0497\n",
            "Epoch 89/1000\n",
            "26/26 - 0s - loss: 0.0448 - val_loss: 0.0321\n",
            "Epoch 90/1000\n",
            "26/26 - 0s - loss: 0.0528 - val_loss: 0.0909\n",
            "Epoch 91/1000\n",
            "26/26 - 0s - loss: 0.0445 - val_loss: 0.0334\n",
            "Epoch 92/1000\n",
            "26/26 - 0s - loss: 0.0333 - val_loss: 0.0301\n",
            "Epoch 93/1000\n",
            "26/26 - 0s - loss: 0.0431 - val_loss: 0.0321\n",
            "Epoch 94/1000\n",
            "26/26 - 0s - loss: 0.0435 - val_loss: 0.0319\n",
            "Epoch 95/1000\n",
            "26/26 - 0s - loss: 0.0318 - val_loss: 0.0303\n",
            "Epoch 96/1000\n",
            "26/26 - 0s - loss: 0.0478 - val_loss: 0.0884\n",
            "Epoch 97/1000\n",
            "26/26 - 0s - loss: 0.0826 - val_loss: 0.0845\n",
            "Epoch 98/1000\n",
            "26/26 - 0s - loss: 0.0705 - val_loss: 0.0409\n",
            "Epoch 99/1000\n",
            "26/26 - 0s - loss: 0.0355 - val_loss: 0.0325\n",
            "Epoch 100/1000\n",
            "26/26 - 0s - loss: 0.0381 - val_loss: 0.0385\n",
            "Epoch 101/1000\n",
            "26/26 - 0s - loss: 0.0337 - val_loss: 0.0316\n",
            "Epoch 102/1000\n",
            "26/26 - 0s - loss: 0.0320 - val_loss: 0.0414\n",
            "Epoch 103/1000\n",
            "26/26 - 0s - loss: 0.0240 - val_loss: 0.0215\n",
            "Epoch 104/1000\n",
            "26/26 - 0s - loss: 0.0341 - val_loss: 0.0483\n",
            "Epoch 105/1000\n",
            "26/26 - 0s - loss: 0.0369 - val_loss: 0.0296\n",
            "Epoch 106/1000\n",
            "26/26 - 0s - loss: 0.0295 - val_loss: 0.0320\n",
            "Epoch 107/1000\n",
            "26/26 - 0s - loss: 0.0281 - val_loss: 0.0187\n",
            "Epoch 108/1000\n",
            "26/26 - 0s - loss: 0.0276 - val_loss: 0.0213\n",
            "Epoch 109/1000\n",
            "26/26 - 0s - loss: 0.0313 - val_loss: 0.0391\n",
            "Epoch 110/1000\n",
            "26/26 - 0s - loss: 0.0341 - val_loss: 0.0275\n",
            "Epoch 111/1000\n",
            "26/26 - 0s - loss: 0.0343 - val_loss: 0.0162\n",
            "Epoch 112/1000\n",
            "26/26 - 0s - loss: 0.0247 - val_loss: 0.0160\n",
            "Epoch 113/1000\n",
            "26/26 - 0s - loss: 0.0479 - val_loss: 0.0381\n",
            "Epoch 114/1000\n",
            "26/26 - 0s - loss: 0.0349 - val_loss: 0.0662\n",
            "Epoch 115/1000\n",
            "26/26 - 0s - loss: 0.0369 - val_loss: 0.0356\n",
            "Epoch 116/1000\n",
            "26/26 - 0s - loss: 0.0260 - val_loss: 0.0193\n",
            "Epoch 117/1000\n",
            "26/26 - 0s - loss: 0.0278 - val_loss: 0.0113\n",
            "Epoch 118/1000\n",
            "26/26 - 0s - loss: 0.0204 - val_loss: 0.0183\n",
            "Epoch 119/1000\n",
            "26/26 - 0s - loss: 0.0931 - val_loss: 0.4148\n",
            "Epoch 120/1000\n",
            "26/26 - 0s - loss: 0.5458 - val_loss: 0.7064\n",
            "Epoch 121/1000\n",
            "26/26 - 0s - loss: 0.1584 - val_loss: 0.0356\n",
            "Epoch 122/1000\n",
            "26/26 - 0s - loss: 0.0214 - val_loss: 0.0107\n",
            "Epoch 123/1000\n",
            "26/26 - 0s - loss: 0.0183 - val_loss: 0.0375\n",
            "Epoch 124/1000\n",
            "26/26 - 0s - loss: 0.0175 - val_loss: 0.0180\n",
            "Epoch 125/1000\n",
            "26/26 - 0s - loss: 0.0163 - val_loss: 0.0094\n",
            "Epoch 126/1000\n",
            "26/26 - 0s - loss: 0.0146 - val_loss: 0.0409\n",
            "Epoch 127/1000\n",
            "26/26 - 0s - loss: 0.0324 - val_loss: 0.0517\n",
            "Epoch 128/1000\n",
            "26/26 - 0s - loss: 0.0268 - val_loss: 0.0170\n",
            "Epoch 129/1000\n",
            "26/26 - 0s - loss: 0.0433 - val_loss: 0.0229\n",
            "Epoch 130/1000\n",
            "26/26 - 0s - loss: 0.0172 - val_loss: 0.0118\n",
            "Epoch 131/1000\n",
            "26/26 - 0s - loss: 0.0149 - val_loss: 0.0142\n",
            "Epoch 132/1000\n",
            "26/26 - 0s - loss: 0.0180 - val_loss: 0.0402\n",
            "Epoch 133/1000\n",
            "26/26 - 0s - loss: 0.0246 - val_loss: 0.0090\n",
            "Epoch 134/1000\n",
            "26/26 - 0s - loss: 0.0261 - val_loss: 0.0112\n",
            "Epoch 135/1000\n",
            "26/26 - 0s - loss: 0.0183 - val_loss: 0.0374\n",
            "Epoch 136/1000\n",
            "26/26 - 0s - loss: 0.0351 - val_loss: 0.0442\n",
            "Epoch 137/1000\n",
            "26/26 - 0s - loss: 0.0267 - val_loss: 0.0360\n",
            "Epoch 138/1000\n",
            "26/26 - 0s - loss: 0.0336 - val_loss: 0.0480\n",
            "Epoch 139/1000\n",
            "26/26 - 0s - loss: 0.0219 - val_loss: 0.0078\n",
            "Epoch 140/1000\n",
            "26/26 - 0s - loss: 0.0118 - val_loss: 0.0176\n",
            "Epoch 141/1000\n",
            "26/26 - 0s - loss: 0.0181 - val_loss: 0.0080\n",
            "Epoch 142/1000\n",
            "26/26 - 0s - loss: 0.0126 - val_loss: 0.0066\n",
            "Epoch 143/1000\n",
            "26/26 - 0s - loss: 0.0107 - val_loss: 0.0089\n",
            "Epoch 144/1000\n",
            "26/26 - 0s - loss: 0.0536 - val_loss: 0.0132\n",
            "Epoch 145/1000\n",
            "26/26 - 0s - loss: 0.0216 - val_loss: 0.0335\n",
            "Epoch 146/1000\n",
            "26/26 - 0s - loss: 0.0660 - val_loss: 0.1201\n",
            "Epoch 147/1000\n",
            "26/26 - 0s - loss: 0.2807 - val_loss: 0.1284\n",
            "Epoch 148/1000\n",
            "26/26 - 0s - loss: 0.0501 - val_loss: 0.0168\n",
            "Epoch 149/1000\n",
            "26/26 - 0s - loss: 0.0332 - val_loss: 0.0270\n",
            "Epoch 150/1000\n",
            "26/26 - 0s - loss: 0.0267 - val_loss: 0.0171\n",
            "Epoch 151/1000\n",
            "26/26 - 0s - loss: 0.0364 - val_loss: 0.0383\n",
            "Epoch 152/1000\n",
            "26/26 - 0s - loss: 0.0269 - val_loss: 0.0527\n",
            "Epoch 153/1000\n",
            "26/26 - 0s - loss: 0.1159 - val_loss: 0.0565\n",
            "Epoch 154/1000\n",
            "26/26 - 0s - loss: 0.0272 - val_loss: 0.0125\n",
            "Epoch 155/1000\n",
            "26/26 - 0s - loss: 0.0128 - val_loss: 0.0106\n",
            "Epoch 156/1000\n",
            "26/26 - 0s - loss: 0.0125 - val_loss: 0.0137\n",
            "Epoch 157/1000\n",
            "26/26 - 0s - loss: 0.0185 - val_loss: 0.0199\n",
            "Epoch 158/1000\n",
            "26/26 - 0s - loss: 0.0218 - val_loss: 0.0097\n",
            "Epoch 159/1000\n",
            "26/26 - 0s - loss: 0.0160 - val_loss: 0.0114\n",
            "Epoch 160/1000\n",
            "26/26 - 0s - loss: 0.0399 - val_loss: 0.0677\n",
            "Epoch 161/1000\n",
            "26/26 - 0s - loss: 0.0493 - val_loss: 0.1542\n",
            "Epoch 162/1000\n",
            "26/26 - 0s - loss: 0.0593 - val_loss: 0.1030\n",
            "Epoch 163/1000\n",
            "26/26 - 0s - loss: 0.0696 - val_loss: 0.0403\n",
            "Epoch 164/1000\n",
            "26/26 - 0s - loss: 0.0202 - val_loss: 0.0318\n",
            "Epoch 165/1000\n",
            "26/26 - 0s - loss: 0.0390 - val_loss: 0.1097\n",
            "Epoch 166/1000\n",
            "26/26 - 0s - loss: 0.1124 - val_loss: 0.1958\n",
            "Epoch 167/1000\n",
            "26/26 - 0s - loss: 0.1114 - val_loss: 0.0367\n",
            "Epoch 168/1000\n",
            "26/26 - 0s - loss: 0.1035 - val_loss: 0.0148\n",
            "Epoch 169/1000\n",
            "26/26 - 0s - loss: 0.1345 - val_loss: 0.0915\n",
            "Epoch 170/1000\n",
            "26/26 - 0s - loss: 0.2927 - val_loss: 0.3278\n",
            "Epoch 171/1000\n",
            "26/26 - 0s - loss: 0.2461 - val_loss: 0.0688\n",
            "Epoch 172/1000\n",
            "26/26 - 0s - loss: 0.0306 - val_loss: 0.0225\n",
            "Epoch 173/1000\n",
            "26/26 - 0s - loss: 0.0174 - val_loss: 0.0106\n",
            "Epoch 174/1000\n",
            "26/26 - 0s - loss: 0.1020 - val_loss: 0.1250\n",
            "Epoch 175/1000\n",
            "26/26 - 0s - loss: 0.0417 - val_loss: 0.0169\n",
            "Epoch 176/1000\n",
            "26/26 - 0s - loss: 0.0281 - val_loss: 0.0192\n",
            "Epoch 177/1000\n",
            "26/26 - 0s - loss: 0.0767 - val_loss: 0.2199\n",
            "Epoch 178/1000\n",
            "26/26 - 0s - loss: 0.1954 - val_loss: 0.0275\n",
            "Epoch 179/1000\n",
            "26/26 - 0s - loss: 0.0496 - val_loss: 0.0314\n",
            "Epoch 180/1000\n",
            "26/26 - 0s - loss: 0.0175 - val_loss: 0.0207\n",
            "Epoch 181/1000\n",
            "26/26 - 0s - loss: 0.0189 - val_loss: 0.0047\n",
            "Epoch 182/1000\n",
            "26/26 - 0s - loss: 0.0079 - val_loss: 0.0044\n",
            "Epoch 183/1000\n",
            "26/26 - 0s - loss: 0.0057 - val_loss: 0.0038\n",
            "Epoch 184/1000\n",
            "26/26 - 0s - loss: 0.0113 - val_loss: 0.0175\n",
            "Epoch 185/1000\n",
            "26/26 - 0s - loss: 0.0172 - val_loss: 0.0231\n",
            "Epoch 186/1000\n",
            "26/26 - 0s - loss: 0.0108 - val_loss: 0.0178\n",
            "Epoch 187/1000\n",
            "26/26 - 0s - loss: 0.0235 - val_loss: 0.0182\n",
            "Epoch 188/1000\n",
            "26/26 - 0s - loss: 0.0280 - val_loss: 0.0414\n",
            "Epoch 189/1000\n",
            "26/26 - 0s - loss: 0.0262 - val_loss: 0.0161\n",
            "Epoch 190/1000\n",
            "26/26 - 0s - loss: 0.0502 - val_loss: 0.1855\n",
            "Epoch 191/1000\n",
            "26/26 - 0s - loss: 0.0455 - val_loss: 0.0354\n",
            "Epoch 192/1000\n",
            "26/26 - 0s - loss: 0.0374 - val_loss: 0.0271\n",
            "Epoch 193/1000\n",
            "26/26 - 0s - loss: 0.0195 - val_loss: 0.0934\n",
            "Epoch 194/1000\n",
            "26/26 - 0s - loss: 0.1459 - val_loss: 0.0163\n",
            "Epoch 195/1000\n",
            "26/26 - 0s - loss: 0.1055 - val_loss: 0.1473\n",
            "Epoch 196/1000\n",
            "26/26 - 0s - loss: 0.4922 - val_loss: 0.3457\n",
            "Epoch 197/1000\n",
            "26/26 - 0s - loss: 0.4576 - val_loss: 0.0595\n",
            "Epoch 198/1000\n",
            "26/26 - 0s - loss: 0.3189 - val_loss: 0.1188\n",
            "Epoch 199/1000\n",
            "26/26 - 0s - loss: 0.1482 - val_loss: 0.0591\n",
            "Epoch 200/1000\n",
            "26/26 - 0s - loss: 0.0614 - val_loss: 0.0439\n",
            "Epoch 201/1000\n",
            "26/26 - 0s - loss: 0.0595 - val_loss: 0.0409\n",
            "Epoch 202/1000\n",
            "26/26 - 0s - loss: 0.0576 - val_loss: 0.0280\n",
            "Epoch 203/1000\n",
            "26/26 - 0s - loss: 0.0725 - val_loss: 0.0819\n",
            "Epoch 204/1000\n",
            "26/26 - 0s - loss: 0.0264 - val_loss: 0.0136\n",
            "Epoch 205/1000\n",
            "26/26 - 0s - loss: 0.0207 - val_loss: 0.0190\n",
            "Epoch 206/1000\n",
            "26/26 - 0s - loss: 0.0299 - val_loss: 0.0212\n",
            "Epoch 207/1000\n",
            "26/26 - 0s - loss: 0.0081 - val_loss: 0.0037\n",
            "Epoch 208/1000\n",
            "26/26 - 0s - loss: 0.0121 - val_loss: 0.0148\n",
            "Epoch 209/1000\n",
            "26/26 - 0s - loss: 0.0109 - val_loss: 0.0061\n",
            "Epoch 210/1000\n",
            "26/26 - 0s - loss: 0.0768 - val_loss: 0.0878\n",
            "Epoch 211/1000\n",
            "26/26 - 0s - loss: 0.3821 - val_loss: 0.0796\n",
            "Epoch 212/1000\n",
            "26/26 - 0s - loss: 0.6065 - val_loss: 0.4948\n",
            "Epoch 213/1000\n",
            "26/26 - 0s - loss: 0.1517 - val_loss: 0.0203\n",
            "Epoch 214/1000\n",
            "26/26 - 0s - loss: 0.0659 - val_loss: 0.0375\n",
            "Epoch 215/1000\n",
            "26/26 - 0s - loss: 0.0283 - val_loss: 0.0710\n",
            "Epoch 216/1000\n",
            "26/26 - 0s - loss: 0.0503 - val_loss: 0.0178\n",
            "Epoch 217/1000\n",
            "26/26 - 0s - loss: 0.0208 - val_loss: 0.0092\n",
            "Epoch 218/1000\n",
            "26/26 - 0s - loss: 0.0119 - val_loss: 0.0172\n",
            "Epoch 219/1000\n",
            "26/26 - 0s - loss: 0.0157 - val_loss: 0.0126\n",
            "Epoch 220/1000\n",
            "26/26 - 0s - loss: 0.0157 - val_loss: 0.0189\n",
            "Epoch 221/1000\n",
            "26/26 - 0s - loss: 0.0257 - val_loss: 0.0259\n",
            "Epoch 222/1000\n",
            "26/26 - 0s - loss: 0.0205 - val_loss: 0.0147\n",
            "Epoch 223/1000\n",
            "26/26 - 0s - loss: 0.0111 - val_loss: 0.0043\n",
            "Epoch 224/1000\n",
            "26/26 - 0s - loss: 0.0055 - val_loss: 0.0095\n",
            "Epoch 225/1000\n",
            "26/26 - 0s - loss: 0.0076 - val_loss: 0.0039\n",
            "Epoch 226/1000\n",
            "26/26 - 0s - loss: 0.0104 - val_loss: 0.0264\n",
            "Epoch 227/1000\n",
            "26/26 - 0s - loss: 0.0155 - val_loss: 0.0160\n",
            "Epoch 228/1000\n",
            "26/26 - 0s - loss: 0.0319 - val_loss: 0.0292\n",
            "Epoch 229/1000\n",
            "26/26 - 0s - loss: 0.0275 - val_loss: 0.0076\n",
            "Epoch 230/1000\n",
            "26/26 - 0s - loss: 0.0177 - val_loss: 0.0231\n",
            "Epoch 231/1000\n",
            "26/26 - 0s - loss: 0.0495 - val_loss: 0.2345\n",
            "Epoch 232/1000\n",
            "26/26 - 0s - loss: 0.1391 - val_loss: 0.0953\n",
            "Epoch 233/1000\n",
            "26/26 - 0s - loss: 0.1085 - val_loss: 0.0145\n",
            "Epoch 234/1000\n",
            "26/26 - 0s - loss: 0.0386 - val_loss: 0.0246\n",
            "Epoch 235/1000\n",
            "26/26 - 0s - loss: 0.0372 - val_loss: 0.0213\n",
            "Epoch 236/1000\n",
            "26/26 - 0s - loss: 0.0375 - val_loss: 0.0848\n",
            "Epoch 237/1000\n",
            "26/26 - 0s - loss: 0.0740 - val_loss: 0.0120\n",
            "Epoch 238/1000\n",
            "26/26 - 0s - loss: 0.0170 - val_loss: 0.0106\n",
            "Epoch 239/1000\n",
            "26/26 - 0s - loss: 0.0125 - val_loss: 0.0294\n",
            "Epoch 240/1000\n",
            "26/26 - 0s - loss: 0.0157 - val_loss: 0.0312\n",
            "Epoch 241/1000\n",
            "26/26 - 0s - loss: 0.0211 - val_loss: 0.0430\n",
            "Epoch 242/1000\n",
            "26/26 - 0s - loss: 0.0386 - val_loss: 0.0403\n",
            "Epoch 243/1000\n",
            "26/26 - 0s - loss: 0.0457 - val_loss: 0.0327\n",
            "Epoch 244/1000\n",
            "26/26 - 0s - loss: 0.2319 - val_loss: 0.2142\n",
            "Epoch 245/1000\n",
            "26/26 - 0s - loss: 0.1144 - val_loss: 0.0309\n",
            "Epoch 246/1000\n",
            "26/26 - 0s - loss: 0.0756 - val_loss: 0.3114\n",
            "Epoch 247/1000\n",
            "26/26 - 0s - loss: 0.0760 - val_loss: 0.0380\n",
            "Epoch 248/1000\n",
            "26/26 - 0s - loss: 0.0339 - val_loss: 0.0079\n",
            "Epoch 249/1000\n",
            "26/26 - 0s - loss: 0.0408 - val_loss: 0.0156\n",
            "Epoch 250/1000\n",
            "26/26 - 0s - loss: 0.0170 - val_loss: 0.0473\n",
            "Epoch 251/1000\n",
            "26/26 - 0s - loss: 0.0893 - val_loss: 0.2889\n",
            "Epoch 252/1000\n",
            "26/26 - 0s - loss: 0.1676 - val_loss: 0.0690\n",
            "Epoch 253/1000\n",
            "26/26 - 0s - loss: 0.2028 - val_loss: 0.0745\n",
            "Epoch 254/1000\n",
            "26/26 - 0s - loss: 0.0672 - val_loss: 0.0711\n",
            "Epoch 255/1000\n",
            "26/26 - 0s - loss: 0.1557 - val_loss: 0.2157\n",
            "Epoch 256/1000\n",
            "26/26 - 0s - loss: 0.2047 - val_loss: 0.1465\n",
            "Epoch 257/1000\n",
            "26/26 - 0s - loss: 0.4282 - val_loss: 0.1441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chief-weapon",
        "outputId": "5d4f2f85-8cd7-4934-d892-598d94e31628"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "id": "chief-weapon",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.003928530961275101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swiss-canvas"
      },
      "source": [
        "# Save the weights\n",
        "model.save_weights('./checkpoints_vision/nn_checkpoint_vision')"
      ],
      "id": "swiss-canvas",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFsfGIir26Z5",
        "outputId": "ba88db8a-593e-4371-fad0-097550bee1d4"
      },
      "source": [
        "!zip -r filename.zip ./checkpoints_vision"
      ],
      "id": "mFsfGIir26Z5",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: checkpoints_vision/ (stored 0%)\n",
            "  adding: checkpoints_vision/nn_checkpoint_vision.index (deflated 58%)\n",
            "  adding: checkpoints_vision/nn_checkpoint_vision.data-00000-of-00001 (deflated 30%)\n",
            "  adding: checkpoints_vision/checkpoint (deflated 51%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mGtt0Pxjmo0",
        "outputId": "a350e4ff-c213-41b4-969a-b1d370bee495"
      },
      "source": [
        "print(model.predict(X_test[0].reshape(1,2)))\n",
        "print(y_test[0])"
      ],
      "id": "7mGtt0Pxjmo0",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.5316721  0.20004682 1.0472419 ]]\n",
            "[1.57   0.3    1.0625]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}